[
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/metamath/lora/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/metamath/loraplus/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/metamath/adalora/profile-paper_math_ift/rank-12to8/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/metamath/pissa/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/magicoder/lora/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/magicoder/loraplus/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/magicoder/adalora/profile-paper_code_ift/rank-12to8/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/magicoder/pissa/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/alpaca/lora/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/alpaca/loraplus/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/alpaca/adalora/profile-paper_default_ift/rank-12to8/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/alpaca/pissa/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/csqa/lora/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/csqa/loraplus/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/csqa/adalora/profile-paper_default_ift/rank-12to8/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "tokenizer_model": "meta-llama/Llama-2-7b-hf",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/meta-llama-Llama-2-7b-hf/csqa/pissa/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/metamath/lora/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/metamath/loraplus/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/metamath/adalora/profile-paper_math_ift/rank-12to8/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "metamath",
    "dataset_id": "meta-math/MetaMathQA",
    "dataset_split": "train",
    "train_profile": "paper_math_ift",
    "max_seq_len": 1024,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 96,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(768 / (1 * 8))",
    "global_train_batch_size_target": 768,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 768,
    "effective_global_batch_size_formula": "96 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/metamath/pissa/profile-paper_math_ift/rank-16/seed42",
    "task_formatting": "Uses first present of query/original_question/question/instruction/prompt as instruction and response/answer/output/solution as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/magicoder/lora/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/magicoder/loraplus/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/magicoder/adalora/profile-paper_code_ift/rank-12to8/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "magicoder",
    "dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K",
    "dataset_split": "train",
    "train_profile": "paper_code_ift",
    "max_seq_len": 4096,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 24,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(192 / (1 * 8))",
    "global_train_batch_size_target": 192,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 192,
    "effective_global_batch_size_formula": "24 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/magicoder/pissa/profile-paper_code_ift/rank-16/seed42",
    "task_formatting": "Uses first present of instruction/prompt/query/problem as instruction and response/output/answer/completion as response; formats with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/alpaca/lora/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/alpaca/loraplus/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/alpaca/adalora/profile-paper_default_ift/rank-12to8/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "alpaca",
    "dataset_id": "tatsu-lab/alpaca",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/alpaca/pissa/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "If text is present, uses text as-is; otherwise uses instruction + optional input + output, formatted with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "lora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/csqa/lora/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (torch.optim) with LoRA+ param groups",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": 20.0,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": "lora_A=lr(0.0002), lora_B=lr*ratio(0.0002*20.0), lora_embedding_A=lr, lora_embedding_B=lr*ratio, other=lr",
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "loraplus",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/csqa/loraplus/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": null,
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": 12,
    "adalora_target_r": 8,
    "adalora_total_step": 200,
    "adalora_tinit": 20,
    "adalora_tfinal": 160,
    "adalora_deltaT": 2,
    "peft_method": "adalora",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/csqa/adalora/profile-paper_default_ift/rank-12to8/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  },
  {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "tokenizer_model": "mistralai/Mistral-7B-v0.1",
    "task": "csqa",
    "dataset_id": "tau/commonsense_qa",
    "dataset_split": "train",
    "train_profile": "paper_default_ift",
    "max_seq_len": 2048,
    "max_steps": 200,
    "num_train_epochs": null,
    "training_schedule": "max_steps",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,
    "gradient_accumulation_source": "global_train_batch_size",
    "gradient_accumulation_formula": "ceil(256 / (1 * 8))",
    "global_train_batch_size_target": 256,
    "assumed_num_gpus": 8,
    "effective_global_batch_size": 256,
    "effective_global_batch_size_formula": "32 * 1 * 8",
    "optimizer_type": "AdamW (transformers default)",
    "optimizer_betas": [
      0.9,
      0.95
    ],
    "weight_decay": 0.0,
    "scheduler_type": "cosine_with_min_lr",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.01,
    "precision_bf16": true,
    "precision_fp16": false,
    "gradient_checkpointing": false,
    "grad_clip": 1.0,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "init_lora_weights": "pissa",
    "pissa_init_mode": "pissa",
    "loraplus_lr_ratio": null,
    "loraplus_lr_embedding": null,
    "loraplus_lr_assignment": null,
    "adalora_init_r": null,
    "adalora_target_r": null,
    "adalora_total_step": null,
    "adalora_tinit": null,
    "adalora_tfinal": null,
    "adalora_deltaT": null,
    "peft_method": "pissa",
    "lr": 0.0002,
    "seed": 42,
    "output_dir": "runs/mistralai-Mistral-7B-v0.1/csqa/pissa/profile-paper_default_ift/rank-16/seed42",
    "task_formatting": "Formats question + choices (A-E) and expects a single-letter answer; wraps with '### Instruction' + '### Response'."
  }
]