{
  "num_total": 16,
  "num_ok": 12,
  "num_bad": 4,
  "bad_reasons_count": {
    "no_pairs_found": 4
  },
  "results": [
    {
      "ok": false,
      "reason": "no_pairs_found",
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/alpaca/adalora/profile-paper_default_ift/rank-12to8/seed42",
      "task": "alpaca",
      "variant": "adalora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/alpaca/lora/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/alpaca/lora/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "alpaca",
      "variant": "lora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/alpaca/loraplus/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/alpaca/loraplus/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "alpaca",
      "variant": "loraplus"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/alpaca/pissa/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/alpaca/pissa/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "alpaca",
      "variant": "pissa"
    },
    {
      "ok": false,
      "reason": "no_pairs_found",
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/csqa/adalora/profile-paper_default_ift/rank-12to8/seed42",
      "task": "csqa",
      "variant": "adalora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/csqa/lora/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/csqa/lora/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "csqa",
      "variant": "lora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/csqa/loraplus/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/csqa/loraplus/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "csqa",
      "variant": "loraplus"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/csqa/pissa/profile-paper_default_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/csqa/pissa/profile-paper_default_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "csqa",
      "variant": "pissa"
    },
    {
      "ok": false,
      "reason": "no_pairs_found",
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/magicoder/adalora/profile-paper_code_ift/rank-12to8/seed42",
      "task": "magicoder",
      "variant": "adalora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/magicoder/lora/profile-paper_code_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/magicoder/lora/profile-paper_code_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "magicoder",
      "variant": "lora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/magicoder/loraplus/profile-paper_code_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/magicoder/loraplus/profile-paper_code_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "magicoder",
      "variant": "loraplus"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/magicoder/pissa/profile-paper_code_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/magicoder/pissa/profile-paper_code_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "magicoder",
      "variant": "pissa"
    },
    {
      "ok": false,
      "reason": "no_pairs_found",
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/metamath/adalora/profile-paper_math_ift/rank-12to8/seed42",
      "task": "metamath",
      "variant": "adalora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/metamath/lora/profile-paper_math_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/metamath/lora/profile-paper_math_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "metamath",
      "variant": "lora"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/metamath/loraplus/profile-paper_math_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/metamath/loraplus/profile-paper_math_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "metamath",
      "variant": "loraplus"
    },
    {
      "ok": true,
      "adapter_dir": "/home/zailongtian/workspace/peft-sft-lab/runs/meta-llama-Llama-2-7b-hf/metamath/pissa/profile-paper_math_ift/rank-16/seed42",
      "out_dir": "/home/zailongtian/workspace/peft-sft-lab/svd_batch/meta-llama-Llama-2-7b-hf/metamath/pissa/profile-paper_math_ift/rank-16/seed42",
      "results_index": {
        "down_proj": {
          "representative_module_norm": "mlp.down_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_down_proj__mlp_down_proj.png",
          "heatmap_V": "heatmap_V_subspace_down_proj__mlp_down_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "gate_proj": {
          "representative_module_norm": "mlp.gate_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_gate_proj__mlp_gate_proj.png",
          "heatmap_V": "heatmap_V_subspace_gate_proj__mlp_gate_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "k_proj": {
          "representative_module_norm": "self_attn.k_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_k_proj__self_attn_k_proj.png",
          "heatmap_V": "heatmap_V_subspace_k_proj__self_attn_k_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "o_proj": {
          "representative_module_norm": "self_attn.o_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_o_proj__self_attn_o_proj.png",
          "heatmap_V": "heatmap_V_subspace_o_proj__self_attn_o_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "q_proj": {
          "representative_module_norm": "self_attn.q_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_q_proj__self_attn_q_proj.png",
          "heatmap_V": "heatmap_V_subspace_q_proj__self_attn_q_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "up_proj": {
          "representative_module_norm": "mlp.up_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_up_proj__mlp_up_proj.png",
          "heatmap_V": "heatmap_V_subspace_up_proj__mlp_up_proj.png",
          "metric": "subspace",
          "extra_curves": []
        },
        "v_proj": {
          "representative_module_norm": "self_attn.v_proj",
          "usable_layers": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31
          ],
          "rank": 16,
          "heatmap_U": "heatmap_U_subspace_v_proj__self_attn_v_proj.png",
          "heatmap_V": "heatmap_V_subspace_v_proj__self_attn_v_proj.png",
          "metric": "subspace",
          "extra_curves": []
        }
      },
      "task": "metamath",
      "variant": "pissa"
    }
  ]
}